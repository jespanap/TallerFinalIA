"""
Taller IA: Aplicaci√≥n Multimodal con OCR y LLMs
Curso: Inteligencia Artificial
Universidad: EAFIT
Profesor: Jorge Padilla
"""

import streamlit as st
import easyocr
from PIL import Image
import numpy as np
from groq import Groq
import os
from dotenv import load_dotenv
import re

from transformers import pipeline

# =============================================================================
# CONFIGURACI√ìN GENERAL
# =============================================================================
load_dotenv(dotenv_path=".env")

st.set_page_config(
    page_title="Taller IA: OCR + LLM",
    page_icon="üß†",
    layout="wide"
)

st.title("üß† Taller IA: OCR + LLM")
st.markdown("### Aplicaci√≥n Multimodal con Visi√≥n Artificial y Procesamiento de Lenguaje Natural")
st.markdown("---")

# =============================================================================
# HELPERS: Hugging Face Pipelines Locales
# =============================================================================
DEFAULT_HF_MAX_TOKENS = 256

def _clean_ticks(s: str) -> str:
    """Limpia horas/timestamps tipo 10:30 y normaliza bullets/espacios."""
    s = re.sub(r"\b\d{1,2}:\d{2}\b", "", s)
    s = re.sub(r"[‚Ä¢\-\u2022]+\s*", "‚Ä¢ ", s)
    s = re.sub(r"\n{3,}", "\n\n", s).strip()
    return s

@st.cache_resource(show_spinner=False)
def _local_summarizer_fast():
    return pipeline("summarization", model="facebook/bart-base")

def hf_summarize(text, max_tokens=DEFAULT_HF_MAX_TOKENS):
    try:
        pipe = _local_summarizer_fast()
        out = pipe(text, max_length=min(256, int(max_tokens)), do_sample=False)
        return out[0]["summary_text"] if out else ""
    except Exception as e:
        st.error(f"Resumen r√°pido fall√≥: {e}")
        return ""

@st.cache_resource(show_spinner=False)
def _local_ner_fast():
    return pipeline(
        "token-classification",
        model="Davlan/distilbert-base-multilingual-cased-ner-hrl",
        aggregation_strategy="simple"
    )

def hf_entities(text, max_tokens=DEFAULT_HF_MAX_TOKENS):
    try:
        ner = _local_ner_fast()
        ents = ner(text)
        cat_map = {
            "PER": "PERSONA",
            "ORG": "ORGANIZACI√ìN",
            "LOC": "LUGAR",
            "MISC": "OTRA",
            "DATE": "FECHA",
        }
        lines = [f"‚Ä¢ [{cat_map.get(e.get('entity_group', 'MISC'))}]: {e.get('word','').strip()}"
                 for e in ents if e.get('word')]
        return "\n".join(lines) if lines else "‚Ä¢ [INFO]: No se detectaron entidades claras."
    except Exception as e:
        st.error(f"NER r√°pido fall√≥: {e}")
        return ""

@st.cache_resource(show_spinner=False)
def _local_translator_es_en_fast():
    return pipeline("translation", model="Helsinki-NLP/opus-mt-es-en")

def hf_translate_to_english(text, max_tokens=DEFAULT_HF_MAX_TOKENS):
    try:
        translator = _local_translator_es_en_fast()
        out = translator(text, max_length=min(256, int(max_tokens)))
        return out[0]["translation_text"] if out else ""
    except Exception as e:
        st.error(f"Traducci√≥n local fall√≥: {e}")
        return ""

# =============================================================================
# M√ìDULO 1: OCR
# =============================================================================
st.header("üì∏ M√≥dulo 1: Extracci√≥n de Texto (OCR)")

@st.cache_resource
def load_ocr_reader():
    return easyocr.Reader(['es', 'en'])

with st.spinner("Cargando modelo OCR..."):
    reader = load_ocr_reader()

uploaded_file = st.file_uploader(
    "Sube una imagen con texto",
    type=['png', 'jpg', 'jpeg'],
    help="Formatos soportados: PNG, JPG, JPEG"
)

if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption="Imagen subida", use_column_width=True)
    image_np = np.array(image)

    if st.button("Extraer Texto", type="primary"):
        with st.spinner("Extrayendo texto de la imagen..."):
            result = reader.readtext(image_np)
            extracted_text = "\n".join([d[1] for d in result])
            st.session_state['extracted_text'] = extracted_text

    if 'extracted_text' in st.session_state:
        st.success("‚úÖ Texto extra√≠do exitosamente")
        st.text_area("Texto extra√≠do:", value=st.session_state['extracted_text'], height=200)

st.markdown("---")

# =============================================================================
# M√ìDULO 2 Y 3: LLMs (GROQ y HUGGING FACE)
# =============================================================================
st.header("üß© M√≥dulo 2 y 3: An√°lisis con Modelos de Lenguaje")

if 'extracted_text' not in st.session_state or not st.session_state['extracted_text']:
    st.info("üëÜ Primero extrae texto de una imagen en la secci√≥n superior.")
else:
    text_input = st.session_state['extracted_text']
    provider = st.radio("Proveedor:", ["GROQ", "Hugging Face"])

    temperature = st.slider("Creatividad (temperature):", 0.0, 2.0, 0.7, 0.1)
    max_tokens = st.slider("M√°x. tokens (longitud):", 50, 2000, 500, 50)
    st.markdown("---")

    if provider == "GROQ":
        st.subheader("üí¨ An√°lisis con GROQ (llama-3.1-8b-instant)")
        groq_api_key = os.getenv("GROQ_API_KEY")
        if not groq_api_key:
            st.error("‚ùå No se encontr√≥ GROQ_API_KEY en .env")
        else:
            task = st.selectbox(
                "Tarea a realizar:",
                ["Resumir texto", "Identificar entidades", "Traducir al ingl√©s"]
            )
            if st.button("Ejecutar an√°lisis GROQ", type="primary"):
                system_prompts = {
                    "Resumir texto": "Resume el siguiente texto en 3 puntos clave concisos:",
                    "Identificar entidades": "Extrae las entidades principales (personas, lugares, organizaciones, fechas):",
                    "Traducir al ingl√©s": "Traduce el siguiente texto al ingl√©s:"
                }
                client = Groq(api_key=groq_api_key)
                try:
                    with st.spinner("Analizando con GROQ..."):
                        chat = client.chat.completions.create(
                            model="llama-3.1-8b-instant",
                            messages=[
                                {"role": "system", "content": system_prompts[task]},
                                {"role": "user", "content": text_input}
                            ],
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        response = chat.choices[0].message.content
                        st.subheader("üß† Respuesta del modelo:")
                        st.write(response)
                        st.info(f"Modelo: llama-3.1-8b-instant | Tarea: {task}")
                except Exception as e:
                    st.error(f"Error al conectar con GROQ: {e}")

    elif provider == "Hugging Face":
        st.subheader("ü§ó An√°lisis con Hugging Face")
        hf_api_key = os.getenv("HUGGINGFACEHUB_API_TOKEN")
        if not hf_api_key:
            st.error("‚ùå No se encontr√≥ HUGGINGFACEHUB_API_TOKEN en .env")
        else:
            task = st.selectbox(
                "Tarea a realizar:",
                ["Resumir texto", "Identificar entidades", "Traducir al ingl√©s"]
            )
            if st.button("Ejecutar an√°lisis Hugging Face", type="primary"):
                try:
                    with st.spinner("Analizando con Hugging Face..."):
                        if task == "Resumir texto":
                            output = hf_summarize(text_input, max_tokens)
                        elif task == "Identificar entidades":
                            output = hf_entities(text_input, max_tokens)
                        elif task == "Traducir al ingl√©s":
                            output = hf_translate_to_english(text_input, max_tokens)
                    st.subheader("üß† Resultado del an√°lisis:")
                    st.write(output)
                    st.info(f"Modelo utilizado: {task}")
                except Exception as e:
                    st.error(f"Error al usar Hugging Face: {e}")

# =============================================================================
# SIDEBAR: Informaci√≥n
# =============================================================================
with st.sidebar:
    st.header(" Informaci√≥n del Proyecto")
    st.markdown("""
    **Taller IA: Aplicaci√≥n Multimodal con OCR y LLMs**
    
    1. Sube una imagen con texto.  
    2. Extrae el texto con OCR.  
    3. Analiza con GROQ o Hugging Face.  

    **Modelos:**
    - GROQ ‚Üí `llama-3.1-8b-instant`
    - Hugging Face ‚Üí  
        üßæ `facebook/bart-base` (resumen)  
        üßç `Davlan/distilbert-base-multilingual-cased-ner-hrl` (entidades)  
        üåç `Helsinki-NLP/opus-mt-es-en` (traducci√≥n)
    """)

    st.markdown("---")
    groq_key = os.getenv("GROQ_API_KEY")
    hf_key = os.getenv("HUGGINGFACEHUB_API_TOKEN")

    if groq_key:
        st.success("GROQ configurado")
    else:
        st.error("GROQ no configurado")

    if hf_key:
        st.success("Hugging Face configurado")
    else:
        st.error("Hugging Face no configurado")
