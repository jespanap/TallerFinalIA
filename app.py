import streamlit as st
import easyocr
from PIL import Image
import numpy as np
from dotenv import load_dotenv
import os
from groq import Groq
from huggingface_hub import InferenceClient

# Cargar variables de entorno
load_dotenv()

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="Taller IA: OCR + LLM", layout="wide")
st.title("ü§ñ Taller IA: OCR + LLM")
st.markdown("---")

# ============================================
# M√ìDULO 1: OCR - LECTOR DE IM√ÅGENES
# ============================================

st.header("üì∑ M√≥dulo 1: Extracci√≥n de Texto (OCR)")

# Funci√≥n para cargar el modelo OCR con cach√©
@st.cache_resource
def cargar_modelo_ocr():
    """Carga el modelo OCR una sola vez y lo mantiene en memoria"""
    reader = easyocr.Reader(['es', 'en'], gpu=False)
    return reader

# Cargar el modelo OCR
with st.spinner("Cargando modelo OCR..."):
    reader = cargar_modelo_ocr()

# Widget para subir imagen
archivo_imagen = st.file_uploader(
    "Sube una imagen con texto",
    type=['png', 'jpg', 'jpeg'],
    help="Formatos soportados: PNG, JPG, JPEG"
)

# Procesar imagen si se ha subido
if archivo_imagen is not None:
    # Mostrar imagen
    imagen = Image.open(archivo_imagen)
    st.image(imagen, caption="Imagen cargada", use_container_width=True)
    
    # Convertir imagen a numpy array para OCR
    imagen_array = np.array(imagen)
    
    # Ejecutar OCR
    with st.spinner("Extrayendo texto de la imagen..."):
        resultado_ocr = reader.readtext(imagen_array)
    
    # Extraer solo el texto
    texto_extraido = " ".join([deteccion[1] for deteccion in resultado_ocr])
    
    # Guardar en session_state para persistencia
    st.session_state['texto_extraido'] = texto_extraido
    
    # Mostrar texto extra√≠do
    st.subheader("üìù Texto Extra√≠do:")
    st.text_area(
        "Texto detectado en la imagen",
        value=texto_extraido,
        height=150,
        key="texto_ocr"
    )
    
    st.success(f"‚úÖ Se extrajeron {len(resultado_ocr)} fragmentos de texto")

st.markdown("---")

# ============================================
# M√ìDULO 2 y 3: AN√ÅLISIS CON LLM
# ============================================

st.header("üß† M√≥dulo 2 y 3: An√°lisis con Modelos de Lenguaje")

# Verificar si hay texto extra√≠do
if 'texto_extraido' in st.session_state and st.session_state['texto_extraido']:
    
    # Crear columnas para mejor organizaci√≥n
    col1, col2 = st.columns(2)
    
    with col1:
        # Selector de proveedor de API
        proveedor = st.radio(
            "Selecciona el proveedor de LLM:",
            ["GROQ", "Hugging Face"],
            horizontal=True
        )
    
    with col2:
        # Selector de tarea
        tarea = st.selectbox(
            "Selecciona la tarea a realizar:",
            [
                "Resumir en 3 puntos clave",
                "Identificar las entidades principales",
                "Traducir al ingl√©s",
                "Analizar sentimiento",
                "Extraer palabras clave"
            ]
        )
    
    # Par√°metros ajustables
    st.subheader("‚öôÔ∏è Par√°metros del Modelo")
    col3, col4 = st.columns(2)
    
    with col3:
        temperature = st.slider(
            "Temperature (Creatividad)",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="Valores bajos (0.1-0.5): respuestas m√°s precisas y determin√≠sticas. Valores altos (0.8-2.0): respuestas m√°s creativas y variadas."
        )
    
    with col4:
        max_tokens = st.slider(
            "Max Tokens (Longitud de respuesta)",
            min_value=50,
            max_value=1000,
            value=300,
            step=50,
            help="Controla la longitud m√°xima de la respuesta generada."
        )
    
    # Opciones espec√≠ficas para GROQ
    if proveedor == "GROQ":
        modelo_groq = st.selectbox(
            "Selecciona el modelo de GROQ:",
            [
                "llama3-8b-8192",
                "mixtral-8x7b-32768",
                "gemma-7b-it"
            ]
        )
    
    # Bot√≥n para analizar
    if st.button("üöÄ Analizar Texto", type="primary"):
        texto_analizar = st.session_state['texto_extraido']
        
        # Construir el prompt seg√∫n la tarea
        prompts_tareas = {
            "Resumir en 3 puntos clave": f"Resume el siguiente texto en exactamente 3 puntos clave. S√© conciso y directo:\n\n{texto_analizar}",
            "Identificar las entidades principales": f"Identifica y lista todas las entidades principales (personas, lugares, organizaciones, fechas) en el siguiente texto:\n\n{texto_analizar}",
            "Traducir al ingl√©s": f"Traduce el siguiente texto al ingl√©s de manera precisa y natural:\n\n{texto_analizar}",
            "Analizar sentimiento": f"Analiza el sentimiento del siguiente texto (positivo, negativo o neutral) y explica por qu√©:\n\n{texto_analizar}",
            "Extraer palabras clave": f"Extrae las 5-10 palabras clave m√°s importantes del siguiente texto:\n\n{texto_analizar}"
        }
        
        prompt_usuario = prompts_tareas[tarea]
        
        # ============================================
        # OPCI√ìN 1: GROQ API
        # ============================================
        if proveedor == "GROQ":
            try:
                with st.spinner(f"Analizando con GROQ ({modelo_groq})..."):
                    # Obtener clave API
                    groq_api_key = os.getenv("GROQ_API_KEY")
                    
                    if not groq_api_key:
                        st.error("‚ùå No se encontr√≥ la clave API de GROQ en el archivo .env")
                    else:
                        # Instanciar cliente de GROQ
                        cliente_groq = Groq(api_key=groq_api_key)
                        
                        # Realizar llamada a la API
                        respuesta = cliente_groq.chat.completions.create(
                            model=modelo_groq,
                            messages=[
                                {
                                    "role": "system",
                                    "content": "Eres un asistente experto en an√°lisis de texto. Responde de manera clara, concisa y profesional."
                                },
                                {
                                    "role": "user",
                                    "content": prompt_usuario
                                }
                            ],
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        
                        # Extraer y mostrar respuesta
                        resultado = respuesta.choices[0].message.content
                        
                        st.subheader("üìä Resultado del An√°lisis (GROQ)")
                        st.markdown(resultado)
                        
                        # Informaci√≥n adicional
                        st.info(f"üîπ Modelo: {modelo_groq} | Temperature: {temperature} | Max Tokens: {max_tokens}")
                        
            except Exception as e:
                st.error(f"‚ùå Error al conectar con GROQ: {str(e)}")
        
        # ============================================
        # OPCI√ìN 2: HUGGING FACE API
        # ============================================
        elif proveedor == "Hugging Face":
            try:
                with st.spinner("Analizando con Hugging Face..."):
                    # Obtener clave API
                    hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
                    
                    if not hf_api_key:
                        st.error("‚ùå No se encontr√≥ la clave API de Hugging Face en el archivo .env")
                    else:
                        # Instanciar cliente de Hugging Face
                        cliente_hf = InferenceClient(token=hf_api_key)
                        
                        # Seleccionar modelo seg√∫n la tarea
                        if tarea == "Resumir en 3 puntos clave":
                            resultado = cliente_hf.summarization(
                                texto_analizar,
                                max_length=max_tokens,
                                min_length=50
                            )
                            resultado_texto = resultado.summary_text
                        else:
                            # Para otras tareas, usar chat completion
                            respuesta = cliente_hf.chat_completion(
                                messages=[
                                    {
                                        "role": "system",
                                        "content": "Eres un asistente experto en an√°lisis de texto."
                                    },
                                    {
                                        "role": "user",
                                        "content": prompt_usuario
                                    }
                                ],
                                max_tokens=max_tokens,
                                temperature=temperature
                            )
                            resultado_texto = respuesta.choices[0].message.content
                        
                        # Mostrar resultado
                        st.subheader("üìä Resultado del An√°lisis (Hugging Face)")
                        st.markdown(resultado_texto)
                        
                        # Informaci√≥n adicional
                        st.info(f"üîπ Proveedor: Hugging Face | Temperature: {temperature} | Max Tokens: {max_tokens}")
                        
            except Exception as e:
                st.error(f"‚ùå Error al conectar con Hugging Face: {str(e)}")
                st.info("üí° Tip: Algunas tareas pueden requerir modelos espec√≠ficos o tener l√≠mites de uso en la API gratuita.")

else:
    st.info("üëÜ Por favor, sube una imagen primero para extraer texto.")

# ============================================
# SECCI√ìN DE INFORMACI√ìN Y REFLEXI√ìN
# ============================================

st.markdown("---")
st.header("üí≠ Puntos de Reflexi√≥n")

with st.expander("ü§î Preguntas para discusi√≥n"):
    st.markdown("""
    ### Diferencias de Velocidad
    - **GROQ**: Optimizado para velocidad extrema, ideal para aplicaciones en tiempo real
    - **Hugging Face**: Mayor variedad de modelos, pero puede ser m√°s lento dependiendo del modelo
    
    ### Efecto de Temperature
    - **Valores bajos (0.1-0.5)**: Respuestas m√°s consistentes y determin√≠sticas. Ideal para tareas t√©cnicas.
    - **Valores altos (0.8-2.0)**: Respuestas m√°s creativas y variadas. √ötil para contenido creativo.
    
    ### Calidad del OCR
    - La calidad del texto extra√≠do afecta directamente la precisi√≥n del an√°lisis
    - Im√°genes claras y con buen contraste producen mejores resultados
    - El idioma y la fuente tipogr√°fica pueden influir en la precisi√≥n
    
    ### Extensiones Posibles
    - An√°lisis de sentimientos en redes sociales
    - Clasificaci√≥n de documentos
    - Generaci√≥n autom√°tica de Q&A
    - Extracci√≥n de datos estructurados
    - Traducci√≥n multiidioma
    """)

with st.expander("üìö Recursos y Documentaci√≥n"):
    st.markdown("""
    - [Documentaci√≥n de Streamlit](https://docs.streamlit.io)
    - [EasyOCR GitHub](https://github.com/JaidedAI/EasyOCR)
    - [GROQ Documentation](https://console.groq.com/docs)
    - [Hugging Face Inference API](https://huggingface.co/docs/api-inference)
    """)