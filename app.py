"""
Taller IA: Aplicaci√≥n Multimodal con OCR y LLMs
Curso: Inteligencia Artificial
Universidad: EAFIT
Profesor: Jorge Padilla
"""

import streamlit as st
import easyocr
from PIL import Image
import numpy as np
from groq import Groq
from transformers import pipeline
import os
from dotenv import load_dotenv

# =============================================================================
# CONFIGURACI√ìN GENERAL
# =============================================================================
load_dotenv(dotenv_path=".env")

st.set_page_config(
    page_title="Taller IA: OCR + LLM",
    page_icon="üß†",
    layout="wide"
)

st.title("üß† Taller IA: OCR + LLM")
st.markdown("### Aplicaci√≥n Multimodal con Visi√≥n Artificial y Procesamiento de Lenguaje Natural")
st.markdown("---")

# =============================================================================
# M√ìDULO 1: OCR
# =============================================================================
st.header("üì∏ M√≥dulo 1: Extracci√≥n de Texto (OCR)")

@st.cache_resource
def load_ocr_reader():
    return easyocr.Reader(['es', 'en'])

with st.spinner("Cargando modelo OCR..."):
    reader = load_ocr_reader()

uploaded_file = st.file_uploader(
    "Sube una imagen con texto",
    type=['png', 'jpg', 'jpeg'],
    help="Formatos soportados: PNG, JPG, JPEG"
)

if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption="Imagen subida", use_column_width=True)
    image_np = np.array(image)

    if st.button("Extraer Texto", type="primary"):
        with st.spinner("Extrayendo texto de la imagen..."):
            result = reader.readtext(image_np)
            extracted_text = "\n".join([d[1] for d in result])
            st.session_state['extracted_text'] = extracted_text

    if 'extracted_text' in st.session_state:
        st.success("‚úÖ Texto extra√≠do exitosamente")
        st.text_area("Texto extra√≠do:", value=st.session_state['extracted_text'], height=200)

st.markdown("---")

# =============================================================================
# M√ìDULO 2 Y 3: LLMs (GROQ y HUGGING FACE)
# =============================================================================
st.header("üß© M√≥dulo 2 y 3: An√°lisis con Modelos de Lenguaje")

if 'extracted_text' not in st.session_state or not st.session_state['extracted_text']:
    st.info("üëÜ Primero extrae texto de una imagen en la secci√≥n superior.")
else:
    text_input = st.session_state['extracted_text']
    provider = st.radio("Proveedor:", ["GROQ", "Hugging Face"])

    # Par√°metros generales
    temperature = st.slider("Creatividad (temperature):", 0.0, 2.0, 0.7, 0.1)
    max_tokens = st.slider("M√°x. tokens (longitud):", 50, 2000, 500, 50)
    st.markdown("---")

    if provider == "GROQ":
        st.subheader("üí¨ An√°lisis con GROQ (llama-3.1-8b-instant)")

        groq_api_key = os.getenv("GROQ_API_KEY")
        if not groq_api_key:
            st.error("‚ùå No se encontr√≥ GROQ_API_KEY en .env")
        else:
            task = st.selectbox(
                "Tarea a realizar:",
                ["Resumir texto", "Identificar entidades", "Traducir al ingl√©s"]
            )

            if st.button("Ejecutar an√°lisis", type="primary"):
                system_prompts = {
                    "Resumir texto": "Resume el siguiente texto en 3 puntos clave concisos:",
                    "Identificar entidades": "Extrae las entidades principales (personas, lugares, organizaciones, fechas):",
                    "Traducir al ingl√©s": "Traduce el siguiente texto al ingl√©s:"
                }

                client = Groq(api_key=groq_api_key)
                try:
                    with st.spinner("Analizando con GROQ..."):
                        chat = client.chat.completions.create(
                            model="llama-3.1-8b-instant",
                            messages=[
                                {"role": "system", "content": system_prompts[task]},
                                {"role": "user", "content": text_input}
                            ],
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        response = chat.choices[0].message.content
                        st.subheader("üß† Respuesta del modelo:")
                        st.write(response)
                        st.info(f"Modelo: llama-3.1-8b-instant | Tarea: {task}")

                except Exception as e:
                    st.error(f"Error al conectar con GROQ: {e}")

    elif provider == "Hugging Face":
        st.subheader("ü§ó An√°lisis con Hugging Face")

        hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        if not hf_api_key:
            st.error("‚ùå No se encontr√≥ HUGGINGFACE_API_KEY en .env")
        else:
            task = st.selectbox(
                "Tarea a realizar:",
                ["Resumir texto", "Identificar entidades", "Traducir al ingl√©s"]
            )

            if st.button("Ejecutar an√°lisis", type="primary"):
                try:
                    with st.spinner("Analizando con Hugging Face..."):
                        if task == "Resumir texto":
                            summarizer = pipeline("summarization", model="facebook/bart-base", token=hf_api_key)
                            result = summarizer(text_input, max_length=100, min_length=25, do_sample=False)
                            output = result[0]["summary_text"]

                        elif task == "Identificar entidades":
                            ner_model = pipeline("ner", model="Davlan/distilbert-base-multilingual-cased-ner-hrl", token=hf_api_key)
                            entities = ner_model(text_input)
                            output = "\n".join([f"{ent['word']} ‚Üí {ent['entity_group']}" for ent in entities])

                        elif task == "Traducir al ingl√©s":
                            translator = pipeline("translation", model="Helsinki-NLP/opus-mt-es-en", token=hf_api_key)
                            translation = translator(text_input)
                            output = translation[0]["translation_text"]

                        st.subheader("üß† Resultado del an√°lisis:")
                        st.write(output)
                        st.info(f"Modelo utilizado: {task}")

                except Exception as e:
                    st.error(f"Error al usar Hugging Face: {e}")

# =============================================================================
# SIDEBAR: Informaci√≥n
# =============================================================================
with st.sidebar:
    st.header(" Informaci√≥n del Proyecto")
    st.markdown("""
    **Taller IA: Aplicaci√≥n Multimodal con OCR y LLMs**
    
    1. Sube una imagen con texto.  
    2. Extrae el texto con OCR.  
    3. Analiza con GROQ o Hugging Face.  

    **Modelos:**
    - GROQ ‚Üí `llama-3.1-8b-instant`
    - Hugging Face ‚Üí  
        üßæ `facebook/bart-base` (resumen)  
        üßç `Davlan/distilbert-base-multilingual-cased-ner-hrl` (entidades)  
        üåç `Helsinki-NLP/opus-mt-es-en` (traducci√≥n)
    """)

    st.markdown("---")
    groq_key = os.getenv("GROQ_API_KEY")
    hf_key = os.getenv("HUGGINGFACE_API_KEY")

    if groq_key:
        st.success("GROQ configurado")
    else:
        st.error("GROQ no configurado")

    if hf_key:
        st.success("Hugging Face configurado")
    else:
        st.error("Hugging Face no configurado")